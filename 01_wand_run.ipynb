{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lr(cb): return cb.pg['lr']\n",
    "def _beta1(cb): return cb.pg['betas'][0]\n",
    "record = RecorderCB(lr = _lr, mom = _beta1)\n",
    "lr = 2e-2\n",
    "epochs = 30\n",
    "\n",
    "# load data\n",
    "\n",
    "tmax = epochs * len(dls.train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Define the baseline sweep configuration\n",
    "sweep_config = {\n",
    "    'name': 'Dynamic Sweep Configuration',\n",
    "    'method': 'grid',  # This example uses grid search; adjust as needed.\n",
    "    'metric': {\n",
    "        'name': 'validation_loss',\n",
    "        'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'data_normalization': {\n",
    "            'values': [False, True]\n",
    "        },\n",
    "        'batch_norm': {\n",
    "            'values': [None, 'nn.BatchNorm2d']  # Placeholder; actual implementation may vary.\n",
    "        },\n",
    "        'activation_function': {\n",
    "            'values': ['relu', {'leaky_relu': 0.1}] \n",
    "        },\n",
    "        'lr_schedule': {\n",
    "            'values': [None, 'cosine_annealing']\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['optim.sgd', 'optim.AdamW']\n",
    "        },\n",
    "        'data_augmentation': {\n",
    "            'values': [False, True]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"your_project_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sweep_config['parameters']['lr_schedule']: sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax) else: sched = None\n",
    "\n",
    "# nn.ReLU else, i might just edit callback to do this\n",
    "astats = ActivationStats(fc.risinstance(GeneralRelu))\n",
    "\n",
    "# if leaky_relu\n",
    "if leaky:\n",
    "    iw = partial(init_weights, leaky=0.1)\n",
    "    act_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\n",
    "else:\n",
    "    iw = init_weights\n",
    "    act_gr = GeneralRelu\n",
    "\n",
    "model = get_model(act_gr,norm=nn.BatchNorm2d).apply(iw)\n",
    "\n",
    "xtra = [BatchSchedCB(sched),record]\n",
    "cbs = [DeviceCB(), ProgressCB(plot=True), MetricsCB(), astats]\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=optim.AdamW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweep ID for 'Step 1 - Baseline Model': \n",
      "Sweep ID for 'Step 2 - data_normalization': \n",
      "Sweep ID for 'Step 3 - batch_norm': \n",
      "Sweep ID for 'Step 4 - activation_function, leak_rate': \n",
      "Sweep ID for 'Step 5 - lr_schedule': \n",
      "Sweep ID for 'Step 6 - optimizer': \n",
      "Sweep ID for 'Step 7 - data_augmentation': \n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "def create_sweep_config(step):\n",
    "    \"\"\"Generates sweep configurations with names based on active parameters.\"\"\"\n",
    "    base_config = {\n",
    "        'method': 'grid',\n",
    "        'metric': {'name': 'validation_loss', 'goal': 'minimize'},\n",
    "        'parameters': {\n",
    "            'epochs': {'value': 50}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Define parameters for each step\n",
    "    steps_parameters = [\n",
    "        {},  # Step 1: Baseline, no additional parameters\n",
    "        {'data_normalization': True},  # Step 2\n",
    "        {'batch_norm': 'nn.BatchNorm2d'},  # Step 3\n",
    "        {'activation_function': 'leaky_relu', 'leak_rate': 0.1},  # Step 4\n",
    "        {'lr_schedule': 'cosine_annealing'},  # Step 5\n",
    "        {'optimizer': 'optim.AdamW'},  # Step 6\n",
    "        {'data_augmentation': True}  # Step 7\n",
    "    ]\n",
    "    \n",
    "    # Update the base configuration with parameters for the current step\n",
    "    for i, params in enumerate(steps_parameters[:step], start=1):\n",
    "        base_config['parameters'].update(params)\n",
    "    \n",
    "    # Generate a unique name based on active parameters\n",
    "    active_params = [key for key in steps_parameters[step-1] if steps_parameters[step-1][key]]\n",
    "    unique_name = \"Step {} - {}\".format(step, \", \".join(active_params))\n",
    "    if not active_params:  # For the baseline model\n",
    "        unique_name = \"Step 1 - Baseline Model\"\n",
    "    \n",
    "    base_config['name'] = unique_name\n",
    "    \n",
    "    return base_config\n",
    "\n",
    "# Example usage: Generate sweep configurations for the first 4 steps\n",
    "for step in range(1, 8):\n",
    "    sweep_config = create_sweep_config(step)\n",
    "    #sweep_id = wandb.sweep(sweep_config, project=\"your_project_name\")\n",
    "    print(f\"Sweep ID for '{sweep_config['name']}': \") #{sweep_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax = epochs * len(dls.train)\n",
    "\n",
    "if sweep_config['parameters']['lr_schedule']: sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax) else: sched = None\n",
    "\n",
    "# this is fine need to check if i can do this with wandb\n",
    "astats = ActivationStats(fc.risinstance(GeneralRelu))\n",
    "\n",
    "# if leaky_relu\n",
    "if leaky:\n",
    "    iw = partial(init_weights, leaky=0.1)\n",
    "    act_gr = partial(GeneralRelu, leak=0.1, sub=0.4)\n",
    "elif init:\n",
    "    iw = init_weights\n",
    "    act_gr = GeneralRelu\n",
    "else:\n",
    "    iw = None\n",
    "    act_gr = GeneralRelu\n",
    "\n",
    "if norm:\n",
    "    norm = nn.BatchNorm2d\n",
    "\n",
    "\n",
    "model = get_model(act_gr,norm=norm).apply(iw) # need to check if i can apply None\n",
    "\n",
    "xtra = [BatchSchedCB(sched),record] # what happens if BatchSChedCB is None\n",
    "cbs = [DeviceCB(), ProgressCB(plot=True), MetricsCB(), astats] # Nothing\n",
    "\n",
    "# just aug\n",
    "learn = TrainLearner(model, dls, F.cross_entropy, lr=lr, cbs=cbs+xtra, opt_func=sweep_config['parameters']['optimizer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rocm6_bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
